---
title: "Thoughts About New lmGC()"
author: "Homer White"
date: "10/03/2014"
output:
  pdf_document:
    fig_height: 3.5
    fig_width: 5
---

# New Version of GC Linear Model?

I'm working on new versions of `lmGC()`.  First I'll source them into this document, along with an example data frame that is not yet in `tigerstats`:

```{r message=FALSE}
require(tigerstats)
require(ggplot2)
source("better_lmGC.R")
source("better_predict_lmGC.R")
source("print_better_lmGC.R")
source("diag_lmGC.R")
load("henderson.rda")
```

# Basic Usage

The new `lmGC()` family of functions would behave a bit differently from the current one.  There would be a new argument `degree` that would allow a polynomial fit.  Also, when you graph the model, we'll use `ggplot2` graphics and we'll include a 95%-prediction ribbon.

The following is a model for Ricky Henderson's season-by-season OBP (on-base percentage):

```{r}
mod <- lmGC2(OBP~Season,data=henderson,degree=2,graph=TRUE)
mod
```

The idea is to tell students about the residual standard error $s$, and point out that when you make a predictions of $y$ values based on a particular known $x$ then the predictions are liable to be off by $s$ or so, and that a rough 68-95 Rule applies:

* about 68% of the time, the real $Y$ will be within an $s$ of your prediction, and
* about 95% of the time it will be within $2s$ of your prediction

But this is only rough.  The graph is trying to show you the more carefully-computed 95% -prediction intervals, which together form the ribbon you see around the fitted curve.

# Numerical Prediction

To get predictions numerically, you still use the generic `predict()` function.  Here's what we predict Ricky Henderson's OBP would have been if he had stayed on for the 2002 season:

```{r}
predict(mod,x=2002)
```

Note that a prediction interval is now provided.  The default level is 95%, but you can change that with the `level` argument.  If you use this with students, point out that the prediction standard error that is provided as the "give-or-take" figure is almost but not quite that same as $s$.  (It will vary a bit depending on the value of $x$.)  Also, the prediction intervals are not really $\hat{y} \pm 2s$.  We would just say to the students that the routine routine is just trying to be a bit less "rough" that than the 68-95 rule was.

# A Simple Model-Selection Criterion

You have an option to check to whether you have a "big-enough" degree fit.  

```{r results='hide'}
lmGC2(OBP~Season,data=henderson,degree=2,check=TRUE)
```

I'm showing just the graph above.  For students the console output would appear as well.

The ribbon switches:  now it is a 95%-confidence band.  For each $x$, the vertical segment of the ribbon over $x$ is the 95%-confidence interval for the expected value of $Y$, given that $X=x$.  If some degree-2 polynomial plus bell-shaped chance variation was the "real" way the data was produced, then you can be pretty confident that if God could somehow swoop down and draw it for you then it would lie mostly within that band.  Now look at the loess curve (or gam curve if the scatter plot has more than 1000 points).  This is your estimate of the curve that generates the data, based "only on the data" and not on the data plus some idea about what form the curve has (e.g., polynomial of a particular degree).

Since the loess curve does not stray too far outside the band, then for all we know the "real curve" is a quadratic and there is probably not much benefit to using a higher-degree curve as a fit.

Compare the reuslts above with what one would get if one checked a linear fit:

```{r}
lmGC2(OBP~Season,data=henderson,check=TRUE)
```  

It's clear now to students: the linear model is too simple, but a quadratic model is probably fine,

Of course one could explore higher-degree fits:

```{r}
lmGC2(OBP~Season,data=henderson,degree=3,graph=TRUE)
```

Note that the $R^2$ is not much higher (and the $s$ is actually worse than at degree two), so there is clearly no benefit in moving to degree 3.

The gigher the degree of the fit, the more over-fitting there will be. This is not always immediatley apparent from the graph (see the following fit):

```{r}
lmGC2(OBP~Season,data=henderson,degree=11,graph=TRUE)
```

However, you can sometimes make the problems more apparent by extrapolating such a small amount that if the model made sense the prediction would make sense:


```{r}
mod2 <- lmGC2(OBP~Season,data=henderson,degree=11)
predict(mod2,x=2002)
```

No way should his OBP increase (esepcially to over 92%) if he had stayed on another year.

Sometimes there are tell-tale signs of problematic models,  The machine may at times issue a warning about "predcitons form rank-deficient matrices", and you may find that some coeeficients are reported as NA.  These are clear signs to reduce the degree of the fit!

**Note*: I have taken the liberty of sclaing the $X$-values prior to computing fits and prediction intervals.  This step prevents the machine from issuing warnings based on numerical computation issues that realted only to the size of the explanaotyr variable values rather than on considerations relevant to model selection.


# Diagnostics

Under the new set-up the argument list for `lmGC()` is getting pretty long.  I'm thinking I would like to move the diagnostics out (they are the least likely topic to cover in MAT 111) and handle them as R does, through the generic `plot()` function:

```{r fig.height=6,fig.width=3.5}
plot(mod)
```



# Feedback?

If you have ideas about how all of this should work, let me know.  Clearly there is a benefit in being able to explore and to fit various models, but there is a big cost in that students will have to be taught to deal with numerical issues (e.g., centering and possibly even re-scaling) so as to separate warnings based on numerical issues from warnings based on stupidity of the chosen model.

Should there rather be a separate, optional function (maybe called `polyFit()`) to handle fits of degree greater than one?

Or should I build re-scaling into `lmGC()`, so that it occurs automatically when numerical issues are going to arise?  I might be able to do so in a way that "hides" the re-scaling from the students.


Also, maybe advise me on timing issues.  I would like to include the new functions in the next CRAN release, which should occur later this Fall, but of course we do not need to install it on the Server this semester if we think it would be confusing to students.  On the other hand it might be helpful for them to take a look near the end of class, as a way of thinking about something like "confidence" intervals in a regression setting, and of course it is empowering to be able to fit a modest curve to data that does look, after all, curvilinear.


